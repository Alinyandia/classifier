{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re\n",
    "import unicodedata\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "%matplotlib inline\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "def normalize(text):\n",
    "    '''Лемматизация + токенизация'''\n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проставляем частеречные теги для каждого слова + заменяем теги на понятные для rusvectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "import requests\n",
    "import re\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/akutuzov/universal-pos-tags/4653e8a9154e93fe2f417c7fdb7a357b7d6ce333/ru-rnc.map'\n",
    "\n",
    "mapping = {}\n",
    "r = requests.get(url, stream=True)\n",
    "for pair in r.text.split('\\n'):\n",
    "    pair = re.sub('\\s+', ' ', pair, flags=re.U).split(' ')\n",
    "    if len(pair) > 1:\n",
    "        mapping[pair[0]] = pair[1]\n",
    "        \n",
    "def tag_mystem(text):  \n",
    "    m = Mystem()\n",
    "    processed = m.analyze(text)\n",
    "    tagged = []\n",
    "    for w in processed:\n",
    "        try:\n",
    "            lemma = w[\"analysis\"][0][\"lex\"].lower().strip()\n",
    "            pos = w[\"analysis\"][0][\"gr\"].split(',')[0]\n",
    "            pos = pos.split('=')[0].strip()\n",
    "            if pos in mapping:\n",
    "                tagged.append(lemma + '_' + mapping[pos]) # здесь мы конвертируем тэги\n",
    "            else:\n",
    "                tagged.append(lemma + '_X') # на случай, если попадется тэг, которого нет в маппинге\n",
    "        except KeyError:\n",
    "            continue # я здесь пропускаю знаки препинания, но вы можете поступить по-другому\n",
    "\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/Users/alinashaymardanova/Downloads/180/model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB Масштабировать вектор до единичной длины + скалярное произведение векторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import matutils\n",
    "\n",
    "def similarity(v1, v2):\n",
    "    v1_norm = matutils.unitvec(np.array(v1))\n",
    "    v2_norm = matutils.unitvec(np.array(v2))\n",
    "    return np.dot(v1_norm, v2_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обращение к rusvectores (т.к. очень времязатратно, то не используем)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def api_similarity(m, w1, w2):\n",
    "    url = '/'.join(['https://rusvectores.org', m, w1 + '__' + w2, 'api', 'similarity/'])\n",
    "    r = requests.get(url, stream=True)\n",
    "    return r.text.split('\\t')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого стихотворения вычисляем попарную близость между каждой парой слов. Используем Евклидову метрику, вычисляем косинус между векторами слов и вычисляем скалярное произведение отмасштабированных векторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sim_vect(tag_teat):\n",
    "    v_sim = list()\n",
    "    v_cos = list()\n",
    "    v_eucl = list()\n",
    "    if len(tag_teat) > 1:\n",
    "        for word in tag_teat:\n",
    "            i = -1\n",
    "            while i <= len(tag_teat):\n",
    "                i += 1\n",
    "                try:\n",
    "                    v_sim.append(similarity(model[word], model[tag_teat[i]]))\n",
    "                    v_eucl.append(distance.euclidean(model[word], model[tag_teat[i]]))\n",
    "                    v_cos.append(cosine(model[word], model[tag_teat[i]]))\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    return  v_sim, v_cos, v_eucl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделяем все наречия и прилагательные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ADV(tag_teat):\n",
    "    adv = [word for word in tag_teat if re.search(\"ADV|ADJ\", word)]\n",
    "    return adv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делим на четверостишия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sanz(text):\n",
    "    n = 5\n",
    "    text = re.sub('\\n \\n', ' \\n\\n', text)\n",
    "    groups = text.split('\\n\\n')\n",
    "    lis = list()\n",
    "    for el in groups:\n",
    "        lis.append(normalize(el))\n",
    "    \n",
    "    \n",
    "    return lis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем вектор документа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w2v_vectors(model, lemmas):    \n",
    "    vec_list = []\n",
    "    lemmas = tag_mystem(''.join(lemmas))\n",
    "    for word in lemmas:\n",
    "        try:\n",
    "            vec = model[word]\n",
    "            vec_list.append(vec)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if len(vec_list) != 0:\n",
    "        return (sum(vec_list) / len(vec_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем вектор четверостишия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sanz_vectors(vectors):\n",
    "    v_sim = list()\n",
    "    v_cos = list()\n",
    "    v_eucl = list()\n",
    "    if len(vectors) != 1:\n",
    "        for word in vectors:\n",
    "            i = -1\n",
    "            while i <= len(vectors):\n",
    "                i += 1\n",
    "                try:\n",
    "                    v_sim.append(similarity(word, vectors[i]))\n",
    "                    v_eucl.append(distance.euclidean(word, vectors[i]))\n",
    "                    v_cos.append(cosine(word, vectors[i]))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    return v_sim, v_cos,v_eucl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-граммы с пробелами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ngram_without_spase(text, N):\n",
    "    \n",
    "    ngram = list()\n",
    "    for word in text.split():\n",
    "        ngr = ngrams(word, N)\n",
    "        \n",
    "        for el in ngr:\n",
    "            ngram.append(el)\n",
    "    \n",
    "    e = collections.Counter(ngram)\n",
    "    \n",
    "    return (e[sorted(e, key=e.get, reverse=True)[0]])/len(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-граммы без пробелов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice, tee\n",
    "\n",
    "def get_ngram(content_norm, N):\n",
    "    three_grams = zip(*(islice(seq, index, None) for index, seq in enumerate(tee(content_norm, N))))\n",
    "    three_grams = [''.join(x) for x in three_grams]\n",
    "     \n",
    "    e = collections.Counter(three_grams)\n",
    "    return (e[sorted(e, key=e.get, reverse=True)[0]])/len(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('/Users/alinashaymardanova/Desktop/person_1.csv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = new_df['poem']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полный анализ корпуса стихотворений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(texts):\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for text in tqdm(texts):\n",
    "        try:\n",
    "            clean_text = normalize(text)\n",
    "            tag_text = tag_mystem(''.join(clean_text))\n",
    "            v_sim, v_cos, v_eucl = get_sim_vect(tag_text)\n",
    "\n",
    "\n",
    "            adv = get_ADV(tag_text)\n",
    "            v_sim_adv, v_cos_adv, v_eucl_adv = get_sim_vect(adv)\n",
    "\n",
    "            lis = get_sanz(text)\n",
    "            vectors = list()\n",
    "            for el in lis:\n",
    "                vectors.append(get_w2v_vectors(model, el))\n",
    "\n",
    "            v_sim_f, v_cos_f, v_eucl_f = get_sanz_vectors(vectors)\n",
    "\n",
    "            bag = collections.Counter(re.findall(r'\\w+', clean_text))\n",
    "            best = sorted(bag, key=bag.get, reverse=True)[:1] \n",
    "            res = bag[''.join(best)]\n",
    "\n",
    "            three = get_ngram(clean_text, 2)\n",
    "            two = get_ngram(clean_text, 3)\n",
    "            \n",
    "            two_gram = get_ngram_without_spase(clean_text, 2)\n",
    "            three_gram = get_ngram_without_spase(clean_text, 3)\n",
    "            two_gram_s = get_ngram(clean_text, 2)\n",
    "            three_gram_s = get_ngram(clean_text, 3)\n",
    "            \n",
    "            \n",
    "            df =  df.append({'Author': 'Person', \n",
    "                             'Poem': text,\n",
    "                             'Semantic_density_dot': np.mean(v_sim),\n",
    "                             'Semantic_density_cos': np.mean(v_cos),\n",
    "                             'Semantic_density_eucl': np.mean(v_eucl),\n",
    "                             'Adj_semantic_density_dot': np.mean(v_sim_adv),\n",
    "                             'Adj_semantic_density_cos': np.mean(v_cos_adv),\n",
    "                             'Adj_semantic_density_eucl': np.mean(v_eucl_adv),\n",
    "                             'Stanzas_dot': np.mean(v_sim_f),\n",
    "                             'Stanzas_cos': np.mean(v_cos_f),\n",
    "                             'Stanzas_eucl': np.mean(v_eucl_f),\n",
    "                             'Frequency word': res,\n",
    "                             'Bigramms_without_spaces': two_gram,\n",
    "                             'ThreeGrams_without_spaces': three_gram,\n",
    "                            'Bigramms_with_spaces': two_gram_s,\n",
    "                            'ThreeGrams_with_spaces': three_gram_s,\n",
    "                            'Len': len(text.split())}, \n",
    "                            ignore_index=True)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anal_df = df['poem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# two_gram = list()\n",
    "# for poe in tqdm(anal_df):\n",
    "#     try:\n",
    "#         clean_text = normalize(poe)\n",
    "#         two_gram.append(get_ngram_without_spase(clean_text, 2))\n",
    "#     except:\n",
    "#         print(poe)\n",
    "        \n",
    "# three_gram = list()\n",
    "# for poe in tqdm(anal_df):\n",
    "#     try:\n",
    "#         clean_text = normalize(poe)\n",
    "#         three_gram.append(get_ngram_without_spase(clean_text, 3))\n",
    "#     except:\n",
    "#         print(poe)\n",
    "\n",
    "# two_gramm = list()\n",
    "\n",
    "# for poe in tqdm(anal_df):\n",
    "#     try:\n",
    "#         clean_text = normalize(poe)\n",
    "#         two_gramm.append(get_ngram(clean_text, 2))\n",
    "#     except:\n",
    "#         print(poe)\n",
    "        \n",
    "        \n",
    "# three_gramm = list()\n",
    "\n",
    "# for poe in tqdm(anal_df):\n",
    "#     try:\n",
    "#         clean_text = normalize(poe)\n",
    "#         three_gramm.append(get_ngram(clean_text, 3))\n",
    "#     except:\n",
    "#         print(poe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Stanzas_dot'].fillna((df['Stanzas_dot'].mean()), inplace=True)\n",
    "df['Stanzas_eucl'].fillna((df['Stanzas_eucl'].mean()), inplace=True)\n",
    "df['Stanzas_sim'].fillna((df['Stanzas_sim'].mean()), inplace=True)\n",
    "df['Adj_semantic_density_dot'].fillna((df['Adj_semantic_density_dot'].mean()), inplace=True)\n",
    "df['Adj_semantic_density_eucl'].fillna((df['Adj_semantic_density_eucl'].mean()), inplace=True)\n",
    "df['Adj_semantic_density_sim'].fillna((df['Adj_semantic_density_sim'].mean()), inplace=True)\n",
    "df['Semantic_density_dot'].fillna((df['Semantic_density_dot'].mean()), inplace=True)\n",
    "df['Semantic_density_eucl'].fillna((df['Semantic_density_eucl'].mean()), inplace=True)\n",
    "df['Semantic_density_sim'].fillna((df['Semantic_density_sim'].mean()), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/Users/alinashaymardanova/Desktop/result.csv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/alinashaymardanova/Desktop/result.csv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Poem</th>\n",
       "      <th>Adj_semantic_density_cos</th>\n",
       "      <th>Adj_semantic_density_eucl</th>\n",
       "      <th>Adj_semantic_density_dot</th>\n",
       "      <th>Semantic_density_cos</th>\n",
       "      <th>Semantic_density_eucl</th>\n",
       "      <th>Semantic_density_dot</th>\n",
       "      <th>Frequency word</th>\n",
       "      <th>Stanzas_dot</th>\n",
       "      <th>Stanzas_eucl</th>\n",
       "      <th>Stanzas_cos</th>\n",
       "      <th>Bigramms_without_spaces</th>\n",
       "      <th>ThreeGrams_without_spaces</th>\n",
       "      <th>Bigramms_with_spaces</th>\n",
       "      <th>ThreeGrams_with_spaces</th>\n",
       "      <th>Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Computer</td>\n",
       "      <td>Он был похож на скрежет тормозов,\\nОна синюшны...</td>\n",
       "      <td>0.594242</td>\n",
       "      <td>20.420713</td>\n",
       "      <td>0.405758</td>\n",
       "      <td>0.883234</td>\n",
       "      <td>35.879446</td>\n",
       "      <td>0.116766</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.505724</td>\n",
       "      <td>10.195531</td>\n",
       "      <td>0.494276</td>\n",
       "      <td>0.046154</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>0.050633</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Computer</td>\n",
       "      <td>Задрав вершины милые штрихи,\\nОтрыв седой равн...</td>\n",
       "      <td>0.567463</td>\n",
       "      <td>22.646386</td>\n",
       "      <td>0.432537</td>\n",
       "      <td>0.877236</td>\n",
       "      <td>33.235625</td>\n",
       "      <td>0.122764</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.505724</td>\n",
       "      <td>10.195531</td>\n",
       "      <td>0.494276</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Computer</td>\n",
       "      <td>Дренажных сверстников сбивая,\\nНога юлила меж ...</td>\n",
       "      <td>0.748438</td>\n",
       "      <td>23.293593</td>\n",
       "      <td>0.251562</td>\n",
       "      <td>0.903880</td>\n",
       "      <td>31.773463</td>\n",
       "      <td>0.096120</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.505724</td>\n",
       "      <td>10.195531</td>\n",
       "      <td>0.494276</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.069307</td>\n",
       "      <td>0.087719</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Computer</td>\n",
       "      <td>Краснощёкий кучер подбоченясь\\nСрезал курящих ...</td>\n",
       "      <td>0.632428</td>\n",
       "      <td>15.204069</td>\n",
       "      <td>0.367572</td>\n",
       "      <td>0.880547</td>\n",
       "      <td>28.207399</td>\n",
       "      <td>0.119453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.505724</td>\n",
       "      <td>10.195531</td>\n",
       "      <td>0.494276</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.021053</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Computer</td>\n",
       "      <td>Импозантные сосульки\\nразукрасили брикеты\\nСоч...</td>\n",
       "      <td>0.748760</td>\n",
       "      <td>24.449864</td>\n",
       "      <td>0.251240</td>\n",
       "      <td>0.872494</td>\n",
       "      <td>30.420635</td>\n",
       "      <td>0.127506</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.505724</td>\n",
       "      <td>10.195531</td>\n",
       "      <td>0.494276</td>\n",
       "      <td>0.063158</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.072072</td>\n",
       "      <td>0.039735</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Author                                               Poem  \\\n",
       "0  Computer  Он был похож на скрежет тормозов,\\nОна синюшны...   \n",
       "1  Computer  Задрав вершины милые штрихи,\\nОтрыв седой равн...   \n",
       "2  Computer  Дренажных сверстников сбивая,\\nНога юлила меж ...   \n",
       "3  Computer  Краснощёкий кучер подбоченясь\\nСрезал курящих ...   \n",
       "4  Computer  Импозантные сосульки\\nразукрасили брикеты\\nСоч...   \n",
       "\n",
       "   Adj_semantic_density_cos  Adj_semantic_density_eucl  \\\n",
       "0                  0.594242                  20.420713   \n",
       "1                  0.567463                  22.646386   \n",
       "2                  0.748438                  23.293593   \n",
       "3                  0.632428                  15.204069   \n",
       "4                  0.748760                  24.449864   \n",
       "\n",
       "   Adj_semantic_density_dot  Semantic_density_cos  Semantic_density_eucl  \\\n",
       "0                  0.405758              0.883234              35.879446   \n",
       "1                  0.432537              0.877236              33.235625   \n",
       "2                  0.251562              0.903880              31.773463   \n",
       "3                  0.367572              0.880547              28.207399   \n",
       "4                  0.251240              0.872494              30.420635   \n",
       "\n",
       "   Semantic_density_dot  Frequency word  Stanzas_dot  Stanzas_eucl  \\\n",
       "0              0.116766             1.0     0.505724     10.195531   \n",
       "1              0.122764             1.0     0.505724     10.195531   \n",
       "2              0.096120             2.0     0.505724     10.195531   \n",
       "3              0.119453             1.0     0.505724     10.195531   \n",
       "4              0.127506             2.0     0.505724     10.195531   \n",
       "\n",
       "   Stanzas_cos  Bigramms_without_spaces  ThreeGrams_without_spaces  \\\n",
       "0     0.494276                 0.046154                   0.032787   \n",
       "1     0.494276                 0.043478                   0.044776   \n",
       "2     0.494276                 0.107527                   0.069307   \n",
       "3     0.494276                 0.058824                   0.031746   \n",
       "4     0.494276                 0.063158                   0.028571   \n",
       "\n",
       "   Bigramms_with_spaces  ThreeGrams_with_spaces  Len  \n",
       "0              0.050633                0.032258   20  \n",
       "1              0.056818                0.028571   17  \n",
       "2              0.087719                0.064516   28  \n",
       "3              0.048780                0.021053   16  \n",
       "4              0.072072                0.039735   25  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, df.isnull().any()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
