{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re\n",
    "import unicodedata\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import ngrams\n",
    "\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "import seaborn as sns\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "mystem = Mystem()\n",
    "\n",
    "SEED=1337\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.cross_validation import train_test_split as train\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/alinashaymardanova/Desktop/Диплом/data/result.csv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df['Unnamed: 0']\n",
    "del df['Frequency word']\n",
    "del df['Poem']\n",
    "del df['Len']\n",
    "del df['Stanzas_cos']\n",
    "del df['Stanzas_eucl']\n",
    "del df['Stanzas_dot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.drop(('Author'), axis=1)\n",
    "y = df['Author']\n",
    "feature_names = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train(X, y, test_size=0.6, random_state = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    random_state=SEED,\n",
    "    max_depth=100,\n",
    "    n_estimators=500,\n",
    "    class_weight='balanced'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "                       criterion='gini', max_depth=100, max_features='auto',\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       n_estimators=500, n_jobs=None, oob_score=False,\n",
       "                       random_state=1337, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = 'рассказ о голубом просторе\\nпоющее ядро зенит\\nмондео задний бампер в сборе\\nразмер аэродромных плит\\nкак заработать в интернете\\nесть ли акулы на пхукете\\nтекст в доме пусто я один\\nвсе о роддоме для мужчин\\nгде в ломоносовском районе\\nизосорбидадинитрат \\nземлеустройство реферат \\nкартошка на мясном бульоне\\nиз мавритании жених \\nтатуировки на двоих'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "import requests\n",
    "import re\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/akutuzov/universal-pos-tags/4653e8a9154e93fe2f417c7fdb7a357b7d6ce333/ru-rnc.map'\n",
    "\n",
    "mapping = {}\n",
    "r = requests.get(url, stream=True)\n",
    "for pair in r.text.split('\\n'):\n",
    "    pair = re.sub('\\s+', ' ', pair, flags=re.U).split(' ')\n",
    "    if len(pair) > 1:\n",
    "        mapping[pair[0]] = pair[1]\n",
    "        \n",
    "def tag_mystem(text):  \n",
    "    m = Mystem()\n",
    "    processed = m.analyze(text)\n",
    "    tagged = []\n",
    "    for w in processed:\n",
    "        try:\n",
    "            lemma = w[\"analysis\"][0][\"lex\"].lower().strip()\n",
    "            pos = w[\"analysis\"][0][\"gr\"].split(',')[0]\n",
    "            pos = pos.split('=')[0].strip()\n",
    "            if pos in mapping:\n",
    "                tagged.append(lemma + '_' + mapping[pos]) # здесь мы конвертируем тэги\n",
    "            else:\n",
    "                tagged.append(lemma + '_X') # на случай, если попадется тэг, которого нет в маппинге\n",
    "        except KeyError:\n",
    "            continue # я здесь пропускаю знаки препинания, но вы можете поступить по-другому\n",
    "\n",
    "    return tagged\n",
    "\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/Users/alinashaymardanova/Downloads/180/model.bin', binary=True)\n",
    "\n",
    "\n",
    "from gensim import matutils\n",
    "\n",
    "def similarity(v1, v2):\n",
    "    v1_norm = matutils.unitvec(np.array(v1))\n",
    "    v2_norm = matutils.unitvec(np.array(v2))\n",
    "    return np.dot(v1_norm, v2_norm)\n",
    "\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "def get_sim_vect(tag_teat):\n",
    "    v_sim = list()\n",
    "    v_cos = list()\n",
    "    v_eucl = list()\n",
    "    if len(tag_teat) > 1:\n",
    "        for word in tag_teat:\n",
    "            i = -1\n",
    "            while i <= len(tag_teat):\n",
    "                i += 1\n",
    "                try:\n",
    "                    v_sim.append(similarity(model[word], model[tag_teat[i]]))\n",
    "                    v_eucl.append(distance.euclidean(model[word], model[tag_teat[i]]))\n",
    "                    v_cos.append(cosine(model[word], model[tag_teat[i]]))\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    return  v_sim, v_cos, v_eucl\n",
    "\n",
    "\n",
    "def get_ADV(tag_teat):\n",
    "    adv = [word for word in tag_teat if re.search(\"ADV|ADJ\", word)]\n",
    "    return adv\n",
    "\n",
    "  \n",
    "\n",
    "def get_sanz_vectors(vectors):\n",
    "    v_sim = list()\n",
    "    v_cos = list()\n",
    "    v_eucl = list()\n",
    "    if len(vectors) != 1:\n",
    "        for word in vectors:\n",
    "            i = -1\n",
    "            while i <= len(vectors):\n",
    "                i += 1\n",
    "                try:\n",
    "                    v_sim.append(similarity(word, vectors[i]))\n",
    "                    v_eucl.append(distance.euclidean(word, vectors[i]))\n",
    "                    v_cos.append(cosine(word, vectors[i]))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    return v_sim, v_cos,v_eucl\n",
    "\n",
    "\n",
    "def get_ngram_without_spase(text, N):\n",
    "    \n",
    "    ngram = list()\n",
    "    for word in text.split():\n",
    "        ngr = ngrams(word, N)\n",
    "        \n",
    "        for el in ngr:\n",
    "            ngram.append(el)\n",
    "    \n",
    "    e = collections.Counter(ngram)\n",
    "    \n",
    "    return (e[sorted(e, key=e.get, reverse=True)[0]])/len(e)\n",
    "\n",
    "\n",
    "from itertools import islice, tee\n",
    "\n",
    "def get_ngram(content_norm, N):\n",
    "    three_grams = zip(*(islice(seq, index, None) for index, seq in enumerate(tee(content_norm, N))))\n",
    "    three_grams = [''.join(x) for x in three_grams]\n",
    "     \n",
    "    e = collections.Counter(three_grams)\n",
    "    return (e[sorted(e, key=e.get, reverse=True)[0]])/len(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analysis(text):\n",
    "    text = input()\n",
    "    df = pd.DataFrame()\n",
    "    try:\n",
    "        clean_text = normalize(text)\n",
    "        tag_text = tag_mystem(''.join(clean_text))\n",
    "        v_sim, v_cos, v_eucl = get_sim_vect(tag_text)\n",
    "\n",
    "\n",
    "        adv = get_ADV(tag_text)\n",
    "        v_sim_adv, v_cos_adv, v_eucl_adv = get_sim_vect(adv)\n",
    "\n",
    "\n",
    "        three = get_ngram(clean_text, 2)\n",
    "        two = get_ngram(clean_text, 3)\n",
    "\n",
    "        two_gram = get_ngram_without_spase(clean_text, 2)\n",
    "        three_gram = get_ngram_without_spase(clean_text, 3)\n",
    "        two_gram_s = get_ngram(clean_text, 2)\n",
    "        three_gram_s = get_ngram(clean_text, 3)\n",
    "\n",
    "\n",
    "        df =  df.append({'Semantic_density_dot': np.mean(v_sim),\n",
    "                         'Semantic_density_cos': np.mean(v_cos),\n",
    "                         'Semantic_density_eucl': np.mean(v_eucl),\n",
    "                         'Adj_semantic_density_dot': np.mean(v_sim_adv),\n",
    "                         'Adj_semantic_density_cos': np.mean(v_cos_adv),\n",
    "                         'Adj_semantic_density_eucl': np.mean(v_eucl_adv),\n",
    "                         'Bigramms_without_spaces': two_gram,\n",
    "                         'ThreeGrams_without_spaces': three_gram,\n",
    "                        'Bigramms_with_spaces': two_gram_s,\n",
    "                        'ThreeGrams_with_spaces': three_gram_s}, \n",
    "                        ignore_index=True)\n",
    "    except:\n",
    "        print('Упс, что-то пошло не так:(')\n",
    "        \n",
    "    return rf.predict(df)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
